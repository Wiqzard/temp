{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wiqzard/temp/blob/main/HUK_interview.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ7Cc9K7GEHw"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oDKhMa6FI6D"
      },
      "outputs": [],
      "source": [
        "!pip install arff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PMUNx2EHCie"
      },
      "source": [
        "## 0. Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVR7eAi8FRi5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import arff\n",
        "\n",
        "data_freq = arff.load('freMTPL2freq.arff')\n",
        "df_freq = pd.DataFrame(data_freq , columns=[\"IDpol\", \"ClaimNb\", \"Exposure\", \"Area\", \"VehPower\", \"VehAge\", \"DrivAge\", \"BonusMalus\", \"VehBrand\", \"VehGas\", \"Density\", \"Region\"] )\n",
        "data_sev = arff.load('freMTPL2sev.arff')\n",
        "df_sev = pd.DataFrame(data_sev, columns=[\"IDpol\", \"ClaimAmount\"])\n",
        "df_sev_agg = df_sev.groupby('IDpol', as_index=False).sum()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_sev.head()\n",
        "df_sev.describe()"
      ],
      "metadata": {
        "id": "x8lo4aIqb5qI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_freq.head()\n",
        "df_freq.describe()"
      ],
      "metadata": {
        "id": "WeVVuiogb-HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.1 Remove Outlier and Create Features"
      ],
      "metadata": {
        "id": "cK2cARvbhsRA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VT7cWUSTFSK3"
      },
      "outputs": [],
      "source": [
        "# Create Features of Interest\n",
        "df = pd.merge(df_freq, df_sev_agg, on='IDpol', how='left')\n",
        "df['ClaimFreq'] = df['ClaimNb'] / df['Exposure']\n",
        "df['ClaimSev'] = df['ClaimAmount']/ df['ClaimNb']\n",
        "df['AnualClaimAmount'] = df['ClaimAmount'] / df['Exposure']\n",
        "\n",
        "# Drop Duplicates with Id in\n",
        "duplicates = df.duplicated()\n",
        "print(f\"Number of duplicate rows: {duplicates.sum()}\")\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "#Outlier Removal Quantile\n",
        "def remove_outlier(data: pd.DataFrame, column: str) -> list[pd.DataFrame, int, int]:\n",
        "    Q1 = data[column][data[column] > 0].quantile(0.25)\n",
        "    Q3 = data[column][data[column] > 0].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    mask = (data[column] >= lower_bound) & (data[column] <= upper_bound) | data[column].isna()\n",
        "    removed = len(data[column])-sum(mask)\n",
        "    print(f\"{column}: LowerBound: {lower_bound:.2f} UpperBound: {upper_bound:.2f} Removed: {removed}\")\n",
        "    return data[mask], lower_bound, upper_bound\n",
        "\n",
        "\n",
        "df_o, a, b = remove_outlier(df, 'ClaimSev')\n",
        "df_o, a, b = remove_outlier(df_o, 'ClaimFreq')\n",
        "df_o = df_o.drop(columns=[\"IDpol\"])\n",
        "\n",
        "# Clip High Vehicle Ages\n",
        "df_o['VehAge'] = df_o['VehAge'].clip(upper=50)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF92ILwHGro2"
      },
      "source": [
        "## 1. Descriptive Analysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_o.head()"
      ],
      "metadata": {
        "id": "9eoMUQGvb2D8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seJwh4DiCGeF"
      },
      "outputs": [],
      "source": [
        "df_o.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98972Y_Z4pzd"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#columns = ['Area', 'VehPower', 'VehAge', 'DrivAge', 'BonusMalus', 'VehBrand', 'VehGas', 'Density', 'Region', 'ClaimAmount', 'ClaimFreq', 'ClaimSev']\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Creating the subplot figure\n",
        "plt.subplot(4, 1, 1)\n",
        "sns.histplot(df_o['ClaimFreq'], kde=False, bins=50)\n",
        "plt.title('Claim Frequency Distribution')\n",
        "\n",
        "plt.subplot(4, 1, 2)\n",
        "sns.histplot(df_o['ClaimFreq'][df_o['ClaimFreq'] > 0], kde=False, bins=50)\n",
        "plt.title('Claim Frequency Distribution (Excl. 0)')\n",
        "\n",
        "plt.subplot(4, 1, 3)\n",
        "sns.histplot(df_o['ClaimSev'], kde=True, bins=50)\n",
        "plt.title('Claim Severity Distribution')\n",
        "\n",
        "plt.subplot(4, 1, 4)\n",
        "sns.histplot(df_o['AnualClaimAmount'], kde=True, bins=50)\n",
        "plt.title('Claim Severity Distribution')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W10unNECG53X"
      },
      "source": [
        "## 2. Feature Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-AByAuzP-Gk"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "df_encoded = df_o.copy()\n",
        "df_encoded['Area'] = df_encoded['Area'].astype('category').cat.codes\n",
        "df_encoded['VehBrand'] = df_encoded['VehBrand'].astype('category').cat.codes\n",
        "df_encoded['VehGas'] = df_encoded['VehGas'].astype('category').cat.codes\n",
        "df_encoded['Region'] = df_encoded['Region'].astype('category').cat.codes\n",
        "\n",
        "# Calculating correlation matrix\n",
        "df_encoded = df_encoded.drop(columns=['ClaimNb', 'Exposure', 'AnualClaimAmount', 'ClaimAmount'])\n",
        "correlation_matrix = df_encoded.corr(method='spearman')\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "heatmap = sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', vmin=-1, vmax=1, linewidths=0.5)\n",
        "plt.title('Correlation Matrix Heatmap')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Response Variable-Predictor Visualization"
      ],
      "metadata": {
        "id": "INNDvFCHbNpt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZQWOL3RnDWA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def calculate_and_plot(freq_df, sev_df, group_col, bins, labels, xlabel,\n",
        "                       plot_predicted=False, pred_claim_freq_col=None, pred_claim_sev_col=None):\n",
        "    freq_df_copy = freq_df.copy()\n",
        "    sev_df_copy = sev_df.copy()\n",
        "\n",
        "    # Calculate the mean claim frequency and average cost for each group\n",
        "    mean_claim_freq = freq_df_copy.groupby(group_col)['ClaimFreq'].mean().reset_index()\n",
        "    mean_avg_cost = sev_df_copy.groupby(group_col)['ClaimSev'].mean().reset_index()\n",
        "\n",
        "    if plot_predicted:\n",
        "        # Calculate the mean predicted values for each group\n",
        "        mean_pred_claim_freq = freq_df_copy.groupby(group_col)[pred_claim_freq_col].mean().reset_index()\n",
        "        mean_pred_avg_cost = sev_df_copy.groupby(group_col)[pred_claim_sev_col].mean().reset_index()\n",
        "\n",
        "    # Calculate the aggregated data for the binned groups\n",
        "    freq_df_copy[f'{group_col}Group'] = pd.cut(freq_df_copy[group_col], bins=bins, labels=labels, right=False)\n",
        "    sev_df_copy[f'{group_col}Group'] = pd.cut(sev_df_copy[group_col], bins=bins, labels=labels, right=False)\n",
        "\n",
        "    agg_data_freq = freq_df_copy.groupby(f'{group_col}Group').agg(\n",
        "        EmpiricalClaimFreq=('ClaimFreq', 'mean'),\n",
        "        Exposure=('Exposure', 'sum')\n",
        "    ).reset_index()\n",
        "\n",
        "    agg_data_sev = sev_df_copy.groupby(f'{group_col}Group').agg(\n",
        "        EmpiricalAvgCost=('ClaimSev', 'mean'),\n",
        "        Exposure=('Exposure', 'sum')\n",
        "    ).reset_index()\n",
        "\n",
        "    #agg_data = pd.merge(agg_data_freq, agg_data_sev, on=f'{group_col}Group')\n",
        "\n",
        "    if plot_predicted:\n",
        "        agg_data_freq['PredictedClaimFreq'] = freq_df_copy.groupby(f'{group_col}Group')[pred_claim_freq_col].mean().values\n",
        "        agg_data_sev['PredictedAvgCost'] = sev_df_copy.groupby(f'{group_col}Group')[pred_claim_sev_col].mean().values\n",
        "    # Plot the figures\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(13, 3))\n",
        "\n",
        "    # Top left plot: Empirical (and Predicted if plot_predicted=True) Annual Claim Frequency vs. Group (Mean for each group)\n",
        "    axes[0].scatter(mean_claim_freq[group_col], mean_claim_freq['ClaimFreq'], color='red', label='Empirical')\n",
        "    if plot_predicted:\n",
        "        axes[0].scatter(mean_pred_claim_freq[group_col], mean_pred_claim_freq[pred_claim_freq_col], color='pink', label='Predicted')\n",
        "    axes[0].set_xlabel(xlabel)\n",
        "    axes[0].set_ylabel('Annual Claim Frequency')\n",
        "    axes[0].legend()\n",
        "\n",
        "    # Top right plot: Empirical (and Predicted if plot_predicted=True) Annual Claim Frequency by Group\n",
        "    axes[1].bar(agg_data_sev[f'{group_col}Group'], agg_data_sev['Exposure'], color='grey', alpha=0.5)\n",
        "    axes[1].set_ylabel('Earned Exposure')\n",
        "    axes[1].set_xlabel(xlabel)\n",
        "    axes2 = axes[1].twinx()\n",
        "    axes2.errorbar(agg_data_sev[f'{group_col}Group'], agg_data_sev['EmpiricalAvgCost'], color='blue', marker='o', linestyle='-', label='Empirical')\n",
        "    if plot_predicted:\n",
        "        #axes2.errorbar(sev_df_copy[\"ClaimSev\"], sev_df_copy[pred_claim_sev_col], color='blue', marker='x', linestyle='--', label='Predicted')\n",
        "        axes2.errorbar(agg_data_sev[f'{group_col}Group'], agg_data_sev['PredictedAvgCost'], color='green', marker='x', linestyle='--', label='Predicted')\n",
        "    axes2.set_ylabel('Annual Claim Severity')\n",
        "    axes2.legend()\n",
        "    #print(agg_data_sev)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Assuming df is the original dataframe you provided\n",
        "df_copy = df_o.copy()\n",
        "\n",
        "# Create separate dataframes for frequency and severity\n",
        "freq_df = df_copy[['DrivAge', 'BonusMalus', 'VehAge', 'ClaimFreq', 'Exposure']].copy()\n",
        "sev_df = df_copy[['DrivAge', 'BonusMalus', 'VehAge', 'ClaimSev', 'Exposure']].copy()\n",
        "\n",
        "groups = [\n",
        "    {'group_col': 'DrivAge', 'bins': [18, 30, 40, 50, 60, 70, 100], 'labels': ['[18,30[', '[30,40[', '[40,50[', '[50,60[', '[60,70[', '[70-['], 'xlabel': 'Driver Age'},\n",
        "    {'group_col': 'BonusMalus', 'bins': [50, 75, 100, 125, 150, 175, 200, 225], 'labels': ['[50,75[', '[75,100[', '[100,125[', '[125,150[', '[150,175[', '[175,200[', '[200,225['], 'xlabel': 'BonusMalus'},\n",
        "    {'group_col': 'VehAge', 'bins': [0, 10, 20, 30, 40, 50], 'labels': ['[0,10[', '[10,20[', '[20,30[', '[30,40[', '[40,50['], 'xlabel': 'Vehicle Age'}\n",
        "]\n",
        "for group in groups:\n",
        "    calculate_and_plot(freq_df, sev_df, group['group_col'], group['bins'], group['labels'], group['xlabel'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp5sVmfxGz_W"
      },
      "source": [
        "## 3. Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Helper Functions"
      ],
      "metadata": {
        "id": "Llbzn1d4arpc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqAEjoUYerti"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.genmod.generalized_linear_model import GLMResults\n",
        "from statsmodels.genmod.families import Poisson, NegativeBinomial, Tweedie, Gamma\n",
        "from statsmodels.discrete.count_model import ZeroInflatedPoisson\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "\n",
        "def fit_and_evaluate_glm(model_family, X_train, y_train, exposure_train, X_test, y_test, exposure_test):\n",
        "    model = sm.GLM(y_train, X_train, family=model_family, var_weights=exposure_train)\n",
        "    results = model.fit()\n",
        "\n",
        "    train_predictions = results.predict(X_train)\n",
        "    test_predictions = results.predict(X_test)\n",
        "\n",
        "    train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "    test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "    train_mse = mean_squared_error(y_train, train_predictions)\n",
        "    test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "    metrics = {\n",
        "        \"train_mae\": train_mae,\n",
        "        \"test_mae\": test_mae,\n",
        "        \"train_mse\": train_mse,\n",
        "        \"test_mse\": test_mse\n",
        "    }\n",
        "\n",
        "    return results, metrics, test_predictions\n",
        "\n",
        "def fit_and_evaluate_zip(X_train, y_train, exposure_train, X_test, y_test, exposure_test):\n",
        "    model = ZeroInflatedPoisson(y_train, X_train, exposure=exposure_train)\n",
        "    results = model.fit(maxiter=70)\n",
        "\n",
        "    train_predictions = results.predict(X_train)\n",
        "    test_predictions = results.predict(X_test)\n",
        "\n",
        "    train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "    test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "    train_mse = mean_squared_error(y_train, train_predictions)\n",
        "    test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "    metrics = {\n",
        "        \"train_mae\": train_mae,\n",
        "        \"test_mae\": test_mae,\n",
        "        \"train_mse\": train_mse,\n",
        "        \"test_mse\": test_mse\n",
        "    }\n",
        "\n",
        "    return results, metrics, test_predictions\n",
        "\n",
        "def fit_and_evaluate_rf(X_train, y_train, X_test, y_test):\n",
        "    model = RandomForestRegressor(n_estimators=100, random_state=69)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    train_predictions = model.predict(X_train)\n",
        "    test_predictions = model.predict(X_test)\n",
        "\n",
        "    train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "    test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "    train_mse = mean_squared_error(y_train, train_predictions)\n",
        "    test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "    metrics = {\n",
        "        \"train_mae\": train_mae,\n",
        "        \"test_mae\": test_mae,\n",
        "        \"train_mse\": train_mse,\n",
        "        \"test_mse\": test_mse\n",
        "    }\n",
        "\n",
        "    return model, metrics, test_predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Dataset Creation"
      ],
      "metadata": {
        "id": "LBt51Ptlaj9-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aaa3fMk5GrI"
      },
      "outputs": [],
      "source": [
        "# Preprocessing and data set preparation\n",
        "model_data = df_o.copy()\n",
        "\n",
        "# Encode Categorical Variables with 1 Hot\n",
        "categorical_columns = ['Area', 'VehBrand', 'VehGas', 'Region']\n",
        "model_data = pd.get_dummies(model_data, columns=categorical_columns, drop_first=True)\n",
        "model_data.columns = model_data.columns.str.replace(\"'\", \"\", regex=True)\n",
        "model_data.columns = model_data.columns.str.replace(\" \", \"_\", regex=True)\n",
        "\n",
        "# Bin Integer Numerical Values\n",
        "model_data[\"VehAge\"] = pd.cut(model_data[\"VehAge\"], bins=10, labels=False)\n",
        "model_data[\"DrivAge\"] = pd.cut(model_data[\"DrivAge\"], bins=10, labels=False)\n",
        "model_data[\"BonusMalus\"] = pd.cut(model_data[\"BonusMalus\"], bins=10, labels=False)\n",
        "\n",
        "model_data[\"ClaimFreq\"] = model_data[\"ClaimFreq\"].astype(float)  # Ensure target is float\n",
        "model_data[\"Exposure\"] = model_data[\"Exposure\"].astype(float)    # Ensure weights are float if needed\n",
        "\n",
        "X = model_data.copy().drop(columns=['ClaimSev', 'ClaimFreq', 'ClaimAmount', 'AnualClaimAmount', 'ClaimNb', 'Exposure'])\n",
        "X = X.astype(int)\n",
        "train_data, test_data, X_train, X_test = train_test_split(model_data, X, test_size=0.2, random_state=69)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Claim Frequency Modeling"
      ],
      "metadata": {
        "id": "jdSHj0u0ZmJW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CA58aY1R5GpB",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Prepare data for ClaimFreq modeling\n",
        "X_train = sm.add_constant(X_train)\n",
        "X_test = sm.add_constant(X_test)\n",
        "y_train_freq = np.asarray(train_data[\"ClaimFreq\"])\n",
        "y_test_freq = np.asarray(test_data[\"ClaimFreq\"])\n",
        "exposure_train = np.asarray(train_data[\"Exposure\"])\n",
        "exposure_test = np.asarray(test_data[\"Exposure\"])\n",
        "\n",
        "# Model families for ClaimFreq\n",
        "model_families = {\n",
        "    \"Poisson\": Poisson(),\n",
        "    \"Negative Binomial\": NegativeBinomial(alpha=1.0),\n",
        "    \"Tweedie\": Tweedie(var_power=1.5)\n",
        "}\n",
        "\n",
        "# Fit GLM models for ClaimFreq and store results\n",
        "models = {}\n",
        "predictions = {}\n",
        "results_summary = {}\n",
        "metrics_summary = {}\n",
        "\n",
        "for name, family in tqdm(model_families.items()):\n",
        "    results, metrics, predictions = fit_and_evaluate_glm(family, X_train, y_train_freq, exposure_train, X_test, y_test_freq, exposure_test)\n",
        "    models[name] = results\n",
        "    predictions[name] = predictions\n",
        "    results_summary[name] = results.summary()\n",
        "    metrics_summary[name] = metrics\n",
        "\n",
        "# Fit ZIP model for ClaimFreq and store results\n",
        "zip_results, zip_metrics, pred = fit_and_evaluate_zip(X_train, y_train_freq, exposure_train, X_test, y_test_freq, exposure_test)\n",
        "results_summary[\"Zero-Inflated Poisson\"] = zip_results.summary()\n",
        "metrics_summary[\"Zero-Inflated Poisson\"] = zip_metrics\n",
        "\n",
        "# Fit Random Forest model for ClaimFreq and store results\n",
        "rf_model_freq, rf_metrics_freq, rf_prediction = fit_and_evaluate_rf(X_train, y_train_freq, X_test, y_test_freq)\n",
        "metrics_summary[\"Random Forest\"] = rf_metrics_freq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display results for ClaimFreq\n",
        "for name, summary in results_summary.items():\n",
        "    print(f\"Model: {name}\\n\")\n",
        "    print(summary)\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Create a DataFrame to display the metrics for ClaimFreq\n",
        "metrics_df = pd.DataFrame(metrics_summary).T\n",
        "metrics_df.columns = [\"Train MAE\", \"Test MAE\", \"Train MSE\", \"Test MSE\"]\n",
        "\n",
        "# Display the metrics DataFrame for ClaimFreq\n",
        "print(\"Claim Frequency Metrics:\")\n",
        "print(metrics_df)"
      ],
      "metadata": {
        "id": "t-NU06xOg2Cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Claim Severity Modeling"
      ],
      "metadata": {
        "id": "6TrCABFrZrgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for ClaimSev modeling\n",
        "train_mask = (train_data['ClaimSev'] > 0) & (~train_data['ClaimSev'].isna())\n",
        "test_mask = (test_data['ClaimSev'] > 0) & (~test_data['ClaimSev'].isna())\n",
        "\n",
        "X_train_sev = sm.add_constant(X_train[train_mask])\n",
        "X_test_sev = sm.add_constant(X_test[test_mask])\n",
        "y_train_sev = np.asarray(train_data[train_mask][\"ClaimSev\"])\n",
        "y_test_sev = np.asarray(test_data[test_mask][\"ClaimSev\"])\n",
        "exposure_train_sev = np.asarray(train_data[train_mask][\"Exposure\"])\n",
        "exposure_test_sev = np.asarray(test_data[test_mask][\"Exposure\"])\n",
        "\n",
        "# Model families for ClaimSev\n",
        "sev_model_families = {\n",
        "    \"Poisson\": Poisson(),\n",
        "    \"Normal\": sm.families.Gaussian(),\n",
        "    \"Negative Binomial\": NegativeBinomial(),\n",
        "    \"Gamma\": Gamma()\n",
        "}\n",
        "\n",
        "# Fit GLM models for ClaimSev and store results\n",
        "sev_models = {}\n",
        "sev_predictions = {}\n",
        "sev_results_summary = {}\n",
        "sev_metrics_summary = {}\n",
        "\n",
        "for name, family in tqdm(sev_model_families.items()):\n",
        "    results, metrics, predictions = fit_and_evaluate_glm(family, X_train_sev, y_train_sev, exposure_train_sev, X_test_sev, y_test_sev, exposure_test_sev)\n",
        "    sev_models[name] = results\n",
        "    sev_predictions[name] = predictions\n",
        "    sev_results_summary[name] = results.summary()\n",
        "    sev_metrics_summary[name] = metrics\n",
        "\n",
        "## Fit Random Forest model for ClaimSev and store results\n",
        "rf_model_sev, rf_metrics_sev, rf_predictions = fit_and_evaluate_rf(X_train_sev, y_train_sev, X_test_sev, y_test_sev)\n",
        "sev_metrics_summary[\"Random Forest\"] = rf_metrics_sev\n",
        "\n"
      ],
      "metadata": {
        "id": "44QBZ_18ZbUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display results for ClaimSev\n",
        "for name, summary in sev_results_summary.items():\n",
        "    print(f\"Model: {name}\\n\")\n",
        "    print(summary)\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "# Create a DataFrame to display the metrics for ClaimSev\n",
        "sev_metrics_df = pd.DataFrame(sev_metrics_summary).T\n",
        "sev_metrics_df.columns = [\"Train MAE\", \"Test MAE\", \"Train MSE\", \"Test MSE\"]\n",
        "\n",
        "# Display the metrics DataFrame for ClaimSev\n",
        "print(\"Claim Severity Metrics:\")\n",
        "print(sev_metrics_df)"
      ],
      "metadata": {
        "id": "HATK0hqMZbRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 Visualization of Results"
      ],
      "metadata": {
        "id": "9AuGK68CaImT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfIeOBgVD0zb"
      },
      "outputs": [],
      "source": [
        "poisson_results, poisson_metrics, freq_predictions = fit_and_evaluate_glm(Poisson(), X_train, y_train_freq, exposure_train, X_test, y_test_freq, exposure_test)\n",
        "gaussian_results, gaussian_metrics, sev_predictions = fit_and_evaluate_glm(sm.families.Gaussian(), X_train_sev, y_train_sev, exposure_train_sev, X_test_sev, y_test_sev, exposure_test_sev)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ea7rzwa_kYhH"
      },
      "outputs": [],
      "source": [
        "combined_freq_data = test_data.copy() #.drop(columns='const')\n",
        "combined_sev_data = test_data.copy().dropna() #.drop(columns='const')\n",
        "\n",
        "combined_freq_data['PredClaimFreq'] = freq_predictions\n",
        "combined_sev_data['PredClaimSev'] = sev_predictions\n",
        "#combined_sev_data = combined_sev_data.dropna()\n",
        "\n",
        "\n",
        "groups = [\n",
        "    {'group_col': 'DrivAge', 'bins': [0, 1, 2, 3, 4, 5, 6], 'labels': ['[18,30[', '[30,40[', '[40,50[', '[50,60[', '[60,70[', '[70-['], 'xlabel': 'Driver Age'},\n",
        "    {'group_col': 'BonusMalus', 'bins': [0, 1, 2, 3, 4, 5, 6, 7], 'labels': ['[50,75[', '[75,100[', '[100,125[', '[125,150[', '[150,175[', '[175,200[', '[200,225['], 'xlabel': 'BonusMalus'},\n",
        "    {'group_col': 'VehAge', 'bins': [0, 1, 2, 3, 4, 5], 'labels': ['[0,10[', '[10,20[', '[20,30[', '[30,40[', '[40,50['], 'xlabel': 'Vehicle Age'}\n",
        "]\n",
        "for group in groups:\n",
        "    calculate_and_plot(combined_freq_data, combined_sev_data, group['group_col'], group['bins'], group['labels'], group['xlabel'],\n",
        "                       plot_predicted=True, pred_claim_freq_col='PredClaimFreq', pred_claim_sev_col='PredClaimSev')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6 Combine Models for Modeling the Average Anual Claim Amount"
      ],
      "metadata": {
        "id": "DB_mvstkaORD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_amount = test_data.copy()\n",
        "test_data_amount.loc[test_data_amount['ClaimNb'] == 0, 'AnualClaimAmount'] = 0\n",
        "test_data_amount = test_data_amount.dropna(subset=['AnualClaimAmount'])\n",
        "X_test_amount = X_test.copy()\n",
        "X_test_amount = X_test_amount.loc[test_data_amount.index]\n",
        "\n",
        "freq_predictions = poisson_results.predict(X_test_amount)\n",
        "sev_predictions = gaussian_results.predict(X_test_amount)\n",
        "\n",
        "predicted_annual_amount = freq_predictions * sev_predictions\n",
        "predicted_annual_amount.describe()"
      ],
      "metadata": {
        "id": "JhCOQkgm0_sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "\n",
        "# This is now only on the ones that already have a claim, we need all and if ClaimNb=0 fill NaN with 0\n",
        "test_data_amount['PredClaimFreq'] = freq_predictions\n",
        "test_data_amount['PredClaimSev'] = sev_predictions\n",
        "test_data_amount['PredAnnualClaimAmount'] = predicted_annual_amount\n",
        "\n",
        "mae = mean_absolute_error(test_data_amount['AnualClaimAmount'], test_data_amount['PredAnnualClaimAmount'])\n",
        "mse = mean_squared_error(test_data_amount['AnualClaimAmount'], test_data_amount['PredAnnualClaimAmount'])\n",
        "mean = (test_data_amount['AnualClaimAmount'] - test_data_amount['PredAnnualClaimAmount']).mean()\n",
        "\n",
        "print(f'MAE: {mae:.2f}')\n",
        "print(f'MSE: {mse:.2f}')\n",
        "print(f'Mean: {mean:.2f}')\n",
        "\n",
        "groups = [\n",
        "    {'group_col': 'DrivAge', 'bins': [0, 1, 2, 3, 4, 5, 6], 'labels': ['[18,30[', '[30,40[', '[40,50[', '[50,60[', '[60,70[', '[70-['], 'xlabel': 'Driver Age'},\n",
        "    {'group_col': 'BonusMalus', 'bins': [0, 1, 2, 3, 4, 5, 6, 7], 'labels': ['[50,75[', '[75,100[', '[100,125[', '[125,150[', '[150,175[', '[175,200[', '[200,225['], 'xlabel': 'BonusMalus'},\n",
        "    {'group_col': 'VehAge', 'bins': [0, 1, 2, 3, 4, 5], 'labels': ['[0,10[', '[10,20[', '[20,30[', '[30,40[', '[40,50['], 'xlabel': 'Vehicle Age'}\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "SMGEB7PwXSF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(len(groups), 1, figsize=(8, 10))\n",
        "\n",
        "for idx, group in enumerate(groups):\n",
        "    group_col = group['group_col']\n",
        "    bins = group['bins']\n",
        "    labels = group['labels']\n",
        "    xlabel = group['xlabel']\n",
        "\n",
        "    test_data_amount[f'{group_col}Group'] = pd.cut(test_data_amount[group_col], bins=bins, labels=labels, right=False)\n",
        "\n",
        "    agg_data_sev = test_data_amount.groupby(f'{group_col}Group').agg({\n",
        "        'PredAnnualClaimAmount': 'mean',\n",
        "        'AnualClaimAmount': 'mean',\n",
        "        'Exposure': 'sum'\n",
        "    }).reset_index()\n",
        "\n",
        "    agg_data_sev['EmpiricalAvgCost'] = agg_data_sev['AnualClaimAmount'] / agg_data_sev['Exposure']\n",
        "    agg_data_sev['PredictedAvgCost'] = agg_data_sev['PredAnnualClaimAmount'] / agg_data_sev['Exposure']\n",
        "\n",
        "    ax1 = axes[idx]\n",
        "    ax1.bar(agg_data_sev[f'{group_col}Group'], agg_data_sev['Exposure'], color='grey', alpha=0.5)\n",
        "    ax1.set_ylabel('Earned Exposure')\n",
        "    ax1.set_xlabel(xlabel)\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.errorbar(agg_data_sev[f'{group_col}Group'], agg_data_sev['EmpiricalAvgCost'], color='blue', marker='o', linestyle='-', label='Empirical')\n",
        "    ax2.errorbar(agg_data_sev[f'{group_col}Group'], agg_data_sev['PredictedAvgCost'], color='green', marker='x', linestyle='--', label='Predicted')\n",
        "    ax2.set_ylabel('Annual Claim Amount')\n",
        "    ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-KPbPCrKkixD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKT_tmxA2ofs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_6CLtac2odW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2PMUNx2EHCie",
        "cK2cARvbhsRA",
        "EF92ILwHGro2",
        "W10unNECG53X",
        "INNDvFCHbNpt",
        "Llbzn1d4arpc",
        "LBt51Ptlaj9-",
        "6TrCABFrZrgN",
        "9AuGK68CaImT",
        "DB_mvstkaORD"
      ],
      "authorship_tag": "ABX9TyPtt+qNMGQW712rRTbDaMgy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}